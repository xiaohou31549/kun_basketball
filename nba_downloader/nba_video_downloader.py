import os
import requests
import logging
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import random
import re
from tqdm import tqdm
import json

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except (ImportError, Exception) as e:
    SELENIUM_AVAILABLE = False
    print(f"Selenium not available, falling back to requests mode: {str(e)}")

from nba_downloader.config import TEAMS, BASE_URL, DOWNLOAD_DIR, PREFERRED_QUALITY, DEBUG, YOU_GET_QUALITY_ARGS
from nba_downloader.video_downloader import VideoDownloader

# é£ä¹¦ webhook URL
FEISHU_WEBHOOK_URL = "https://open.feishu.cn/open-apis/bot/v2/hook/453de894-fdba-4a72-aa01-21b8b756d2e1"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG if DEBUG else logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(DOWNLOAD_DIR, 'nba_downloader.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class NBAVideoDownloader:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.use_selenium = True
        self.driver = None
        self.video_downloader = VideoDownloader(DOWNLOAD_DIR, YOU_GET_QUALITY_ARGS)
        self.download_results = {
            'matches': [],  # æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ¯”èµ›
            'success': [],  # æˆåŠŸä¸‹è½½çš„æ¯”èµ›
            'failed': [],   # ä¸‹è½½å¤±è´¥çš„æ¯”èµ›
            'errors': {}    # å¤±è´¥åŸå› 
        }
        self.init_selenium()

    def init_selenium(self):
        """åˆå§‹åŒ–Selenium"""
        try:
            chrome_options = Options()
            if not DEBUG:
                chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument(f'user-agent={self.session.headers["User-Agent"]}')

            # ç¦ç”¨webdriverç‰¹å¾
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    })
                '''
            })
            logger.info("Selenium initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Selenium: {str(e)}")
            self.use_selenium = False

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass

    def get_yesterday_dates(self):
        """è·å–å‰ä¸€å¤©çš„æ—¥æœŸï¼Œè¿”å›å¤šç§æ ¼å¼ä»¥ä¾¿åŒ¹é…"""
        yesterday = datetime.now() - timedelta(days=1)
        # è¿”å›å¤šç§å¯èƒ½çš„æ—¥æœŸæ ¼å¼
        dates = {
            'standard': yesterday.strftime('%mæœˆ%dæ—¥'),  # 01æœˆ05æ—¥ï¼Œç”¨äºåŒ¹é…
            'display': yesterday.strftime('%-mæœˆ%-då·'),  # 1æœˆ5å·ï¼Œç”¨äºæ˜¾ç¤º
            'numeric': yesterday.strftime('%m%d')  # 0105
        }
        logger.debug(f"Generated yesterday dates: {dates}")
        return dates

    def format_date(self, date_text):
        """æ ¼å¼åŒ–æ—¥æœŸæ–‡æœ¬ï¼Œå»æ‰å‰å¯¼é›¶"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ—¥æœŸæ ¼å¼
        match = re.match(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', date_text)
        if match:
            month, day = match.groups()
            # å°†å­—ç¬¦ä¸²è½¬ä¸ºæ•´æ•°å»æ‰å‰å¯¼é›¶ï¼Œå†è½¬å›å­—ç¬¦ä¸²
            return f"{int(month)}æœˆ{int(day)}å·"
        return date_text

    def extract_date_from_title(self, title):
        """ä»æ ‡é¢˜ä¸­æå–æ—¥æœŸ"""
        try:
            # åŒ¹é…æ ¼å¼ï¼š01æœˆ05æ—¥ æˆ– 1æœˆ5æ—¥
            date_match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', title)
            if date_match:
                month, day = date_match.groups()
                # ä¿æŒå¸¦å‰å¯¼é›¶çš„æ ¼å¼ç”¨äºåŒ¹é…
                return f"{int(month):02d}æœˆ{int(day):02d}æ—¥"
            return None
        except Exception as e:
            logger.error(f"Error extracting date from title: {str(e)}")
            return None

    def create_match_directory(self, date_text, title):
        """åˆ›å»ºæ¯”èµ›ç›®å½•"""
        # ä»æ ‡é¢˜ä¸­æå–çƒé˜Ÿä¿¡æ¯
        teams = []
        for team in TEAMS:
            if team in title:
                teams.append(team)
        
        if len(teams) >= 2:
            teams_str = f"{teams[0]}vs{teams[1]}"
        else:
            teams_str = title
            
        # åˆ›å»ºç®€æ´çš„ç›®å½•å: 1æœˆ5å·ç°ç†Švså‹‡å£«
        formatted_date = self.format_date(date_text)  # æ ¼å¼åŒ–æ—¥æœŸï¼Œå»æ‰å‰å¯¼é›¶
        dir_name = f"{formatted_date}{teams_str}"
        dir_name = re.sub(r'[<>:"/\\|?*]', '', dir_name)  # ç§»é™¤éæ³•å­—ç¬¦
        return os.path.join(DOWNLOAD_DIR, dir_name)

    def is_yesterday_match(self, date_text):
        """æ£€æŸ¥æ˜¯å¦æ˜¯æ˜¨å¤©çš„æ¯”èµ›"""
        if not date_text:
            return False
            
        yesterday_dates = self.get_yesterday_dates()
        logger.debug(f"Comparing date_text '{date_text}' with yesterday_dates {yesterday_dates}")
        
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼è¿›è¡Œæ¯”è¾ƒ
        try:
            match = re.match(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', date_text)
            if match:
                month, day = match.groups()
                normalized_date = f"{int(month):02d}æœˆ{int(day):02d}æ—¥"
                return normalized_date == yesterday_dates['standard']
        except Exception as e:
            logger.error(f"Error comparing dates: {str(e)}")
            return False
            
        return False

    def is_team_match(self, match_title):
        """æ£€æŸ¥æ¯”èµ›æ˜¯å¦åŒ…å«å…³æ³¨çš„çƒé˜Ÿ"""
        return any(team in match_title for team in TEAMS)

    def get_page_content(self, url, use_selenium=False):
        """è·å–é¡µé¢å†…å®¹"""
        logger.info(f"Fetching URL: {url}")
        try:
            if use_selenium and self.use_selenium:
                self.driver.get(url)
                time.sleep(random.uniform(2, 3))  # ç­‰å¾…é¡µé¢åŠ è½½
                content = self.driver.page_source
                logger.debug("Page fetched using Selenium")
            else:
                response = self.session.get(url)
                response.raise_for_status()
                content = response.text
                logger.debug("Page fetched using requests")

            if DEBUG:
                logger.debug(f"Content length: {len(content)}")
            return content
        except Exception as e:
            logger.error(f"Error fetching page {url}: {str(e)}")
            return None

    def get_video_url(self, detail_url):
        """ä»è¯¦æƒ…é¡µè·å–è§†é¢‘URL"""
        try:
            content = self.get_page_content(detail_url)
            if not content:
                return None
                
            soup = BeautifulSoup(content, 'html.parser')
            video_links = []
            
            # æŸ¥æ‰¾å¾®åšé“¾æ¥
            for a in soup.select('#lx li.cd a, #jj li.cd a'):
                if 'å¾®åš' in a.text:
                    quarter = None
                    for q in ['ç¬¬ä¸€èŠ‚', 'ç¬¬äºŒèŠ‚', 'ç¬¬ä¸‰èŠ‚', 'ç¬¬å››èŠ‚']:
                        if q in a.text:
                            quarter = q
                            break
                    
                    weibo_url = a['href']
                    logger.info(f"Found Weibo video: {weibo_url}")
                    
                    video_links.append({
                        'type': 'weibo',
                        'url': weibo_url,
                        'text': a.text,
                        'quarter': quarter,
                        'priority': 2 if 'å›½è¯­' in a.text else 1
                    })
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºè§†é¢‘é“¾æ¥ï¼ˆå¾®åšå›½è¯­ > å¾®åšæ™®é€šï¼‰
            video_links.sort(key=lambda x: (-x['priority'], x['quarter'] or ''))
            return video_links
            
        except Exception as e:
            logger.error(f"Error getting video URL from detail page: {str(e)}")
            return None

    def convert_quarter_name(self, quarter_text):
        """å°†ä¸­æ–‡èŠ‚æ•°è½¬æ¢ä¸ºæ•°å­—"""
        quarter_map = {
            'ç¬¬ä¸€èŠ‚': '1',
            'ç¬¬äºŒèŠ‚': '2',
            'ç¬¬ä¸‰èŠ‚': '3',
            'ç¬¬å››èŠ‚': '4',
            'åŠ æ—¶': 'OT'
        }
        return quarter_map.get(quarter_text, quarter_text)

    def process_match(self, match):
        """å¤„ç†å•åœºæ¯”èµ›"""
        logger.info(f"Processing match: {match['title']}")
        
        try:
            # åˆ›å»ºæ¯”èµ›ç›®å½•
            match_dir = self.create_match_directory(match['date'], match['title'])
            
            # è·å–è§†é¢‘é“¾æ¥
            video_links = self.get_video_url(match['url'])
            if not video_links:
                error_msg = "æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥"
                logger.error(error_msg)
                self.download_results['failed'].append(match['title'])
                self.download_results['errors'][match['title']] = error_msg
                return False

            # æå–çƒé˜Ÿä¿¡æ¯ç”¨äºæ–‡ä»¶å
            teams = []
            for team in TEAMS:
                if team in match['title']:
                    teams.append(team)
            
            base_filename = f"{teams[0]}vs{teams[1]}" if len(teams) >= 2 else match['title']
            
            # ä¸‹è½½è§†é¢‘
            success = True
            for video_info in video_links:
                if 'èŠ‚' in video_info.get('text', ''):
                    # å°†ä¸­æ–‡èŠ‚æ•°è½¬æ¢ä¸ºæ•°å­—: ç°ç†Švså‹‡å£«_ç¬¬1èŠ‚
                    quarter = re.search(r'ç¬¬[ä¸€äºŒä¸‰å››]èŠ‚|åŠ æ—¶', video_info['text'])
                    if quarter:
                        quarter_num = self.convert_quarter_name(quarter.group())
                        filename = f"{base_filename}_ç¬¬{quarter_num}èŠ‚"
                    else:
                        filename = f"{base_filename}_{video_info['text']}"
                else:
                    filename = base_filename

                if not self.video_downloader.download(video_info, match_dir, filename, PREFERRED_QUALITY):
                    error_msg = f"ä¸‹è½½å¤±è´¥: {video_info.get('text', '')}"
                    logger.error(error_msg)
                    success = False
                    if match['title'] not in self.download_results['failed']:
                        self.download_results['failed'].append(match['title'])
                        self.download_results['errors'][match['title']] = error_msg

            if success:
                self.download_results['success'].append(match['title'])
            return success

        except Exception as e:
            error_msg = str(e)
            logger.error(f"å¤„ç†æ¯”èµ›æ—¶å‘ç”Ÿé”™è¯¯: {error_msg}")
            self.download_results['failed'].append(match['title'])
            self.download_results['errors'][match['title']] = error_msg
            return False

    def send_feishu_message(self):
        """å‘é€é£ä¹¦æ¶ˆæ¯"""
        # è·å–æ˜¨å¤©çš„æ—¥æœŸ
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Yå¹´%mæœˆ%dæ—¥')
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        message = f"ğŸ€ NBAæ¯”èµ›ä¸‹è½½æŠ¥å‘Š ({yesterday})\n\n"
        
        # æ·»åŠ ç¬¦åˆè¦æ±‚çš„æ¯”èµ›
        message += "ğŸ“… ç¬¦åˆä¸‹è½½è¦æ±‚çš„æ¯”èµ›ï¼š\n"
        if self.download_results['matches']:
            for match in self.download_results['matches']:
                message += f"â€¢ {match}\n"
        else:
            message += "æ— \n"
        
        # æ·»åŠ æˆåŠŸä¸‹è½½çš„æ¯”èµ›
        message += "\nâœ… ä¸‹è½½æˆåŠŸï¼š\n"
        if self.download_results['success']:
            for match in self.download_results['success']:
                message += f"â€¢ {match}\n"
        else:
            message += "æ— \n"
        
        # æ·»åŠ ä¸‹è½½å¤±è´¥çš„æ¯”èµ›
        message += "\nâŒ ä¸‹è½½å¤±è´¥ï¼š\n"
        if self.download_results['failed']:
            for match in self.download_results['failed']:
                reason = self.download_results['errors'].get(match, "æœªçŸ¥åŸå› ")
                message += f"â€¢ {match}\n  åŸå› ï¼š{reason}\n"
        else:
            message += "æ— \n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total = len(self.download_results['matches'])
        success = len(self.download_results['success'])
        if total > 0:
            success_rate = (success / total) * 100
            message += f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š\n"
            message += f"æ€»åœºæ¬¡ï¼š{total}\n"
            message += f"æˆåŠŸï¼š{success}\n"
            message += f"æˆåŠŸç‡ï¼š{success_rate:.1f}%\n"
        
        # å‘é€æ¶ˆæ¯åˆ°é£ä¹¦
        try:
            payload = {
                "msg_type": "text",
                "content": {
                    "text": message
                }
            }
            
            response = requests.post(FEISHU_WEBHOOK_URL, json=payload)
            response.raise_for_status()
            logger.info("æˆåŠŸå‘é€é£ä¹¦æ¶ˆæ¯")
        except Exception as e:
            logger.error(f"å‘é€é£ä¹¦æ¶ˆæ¯å¤±è´¥: {str(e)}")

    def get_matches(self):
        """è·å–æ¯”èµ›åˆ—è¡¨"""
        content = self.get_page_content(BASE_URL, use_selenium=True)
        if not content:
            return []
            
        soup = BeautifulSoup(content, 'html.parser')
        matches = []
        
        # è·å–æ˜¨å¤©çš„æ—¥æœŸ
        yesterday_dates = self.get_yesterday_dates()
        logger.info(f"Looking for matches from dates: {yesterday_dates}")
        
        # ä¿å­˜é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•
        if DEBUG:
            debug_file = os.path.join(DOWNLOAD_DIR, 'debug_page.html')
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.debug(f"Saved page content to {debug_file}")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ¯”èµ›
        for match_item in soup.select('.wrap-body li.c'):
            try:
                # è·å–æ—¥æœŸ
                date_elem = match_item.select_one('em')
                if not date_elem:
                    logger.debug("No date element found")
                    continue
                date_text = date_elem.text.strip()
                logger.debug(f"Found date: {date_text}")
                
                # è·å–æ ‡é¢˜å’Œé“¾æ¥
                link_elem = match_item.select_one('a[href*="/video-"]')
                if not link_elem:
                    logger.debug("No link element found")
                    continue
                    
                title = link_elem.text.strip()
                logger.debug(f"Found title: {title}")
                
                # ä»æ ‡é¢˜ä¸­æå–æ—¥æœŸå’Œçƒé˜Ÿ
                title_parts = title.split(' ')
                if len(title_parts) < 4:  # ç¡®ä¿æ ‡é¢˜æ ¼å¼æ­£ç¡®
                    continue
                    
                match_date = title_parts[0]  # ä¾‹å¦‚ï¼š"01æœˆ01æ—¥"
                logger.debug(f"Extracted date from title: {match_date}")
                
                # æå–çƒé˜Ÿåç§°
                teams = []
                for team in TEAMS:
                    if team in title:
                        teams.append(team)
                
                # å¦‚æœæ‰¾åˆ°äº†ä¸¤æ”¯çƒé˜Ÿï¼Œåˆ›å»ºä¸€ä¸ªæ›´ç®€æ´çš„æ ‡é¢˜
                if len(teams) >= 2:
                    simplified_title = f"{teams[0]}vs{teams[1]}"
                else:
                    # å°è¯•ä»æ ‡é¢˜ä¸­æå–çƒé˜Ÿåç§°ï¼ˆé€šå¸¸åœ¨"vs"å‘¨å›´ï¼‰
                    vs_index = title.find('vs')
                    if vs_index != -1:
                        # åœ¨"vs"å‰åæŸ¥æ‰¾ç©ºæ ¼æ¥æå–çƒé˜Ÿåç§°
                        before_vs = title[:vs_index].strip().split(' ')[-1]
                        after_vs = title[vs_index+2:].strip().split(' ')[0]
                        simplified_title = f"{before_vs}vs{after_vs}"
                    else:
                        simplified_title = title
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ˜¨å¤©çš„æ¯”èµ›å’Œå…³æ³¨çš„çƒé˜Ÿ
                is_yesterday = self.is_yesterday_match(match_date)
                is_team = self.is_team_match(title)
                logger.debug(f"Is yesterday match: {is_yesterday}, Is team match: {is_team}")
                
                if is_yesterday and is_team:
                    detail_url = urljoin(BASE_URL, link_elem['href'])
                    matches.append({
                        'title': simplified_title,
                        'url': detail_url,
                        'date': match_date
                    })
                    logger.info(f"Found match: {simplified_title} ({match_date})")
            
            except Exception as e:
                logger.error(f"Error parsing match item: {str(e)}")
                continue
        
        logger.info(f"Total matches found: {len(matches)}")
        return matches

    def run(self):
        """è¿è¡Œä¸‹è½½å™¨"""
        try:
            logger.info("Starting NBA video downloader")
            logger.info(f"Base URL: {BASE_URL}")
            logger.info(f"Download directory: {DOWNLOAD_DIR}")
            logger.info(f"Teams to track: {TEAMS}")

            # è·å–æ¯”èµ›åˆ—è¡¨
            matches = self.get_matches()
            if not matches:
                logger.info("No matches found")
                self.send_feishu_message()  # å³ä½¿æ²¡æœ‰æ¯”èµ›ä¹Ÿå‘é€æ¶ˆæ¯
                return

            # è®°å½•æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ¯”èµ›
            self.download_results['matches'] = [match['title'] for match in matches]
            
            total_matches = len(matches)
            successful_downloads = 0

            for match in matches:
                if self.process_match(match):
                    successful_downloads += 1

            # æ·»åŠ æ€»ç»“æ—¥å¿—
            logger.info("=== ä¸‹è½½ä»»åŠ¡æ€»ç»“ ===")
            logger.info(f"ä»Šæ—¥ç¬¦åˆè¦æ±‚çš„æ¯”èµ›æ•°é‡: {total_matches}")
            logger.info(f"æˆåŠŸä¸‹è½½çš„æ¯”èµ›æ•°é‡: {successful_downloads}")
            logger.info(f"ä¸‹è½½æˆåŠŸç‡: {(successful_downloads/total_matches*100):.1f}% å¦‚æœæˆåŠŸç‡è¾ƒä½ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¸­çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯")
            logger.info("================")

            # å‘é€é£ä¹¦æ¶ˆæ¯
            self.send_feishu_message()

        except Exception as e:
            logger.error(f"Error running downloader: {str(e)}")
            if DEBUG:
                raise

def main():
    """Entry point for the application."""
    downloader = NBAVideoDownloader()
    downloader.run()

if __name__ == '__main__':
    main()
